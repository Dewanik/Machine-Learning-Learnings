{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewanik/RNN_from_scratch/blob/master/Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uzr1f68HH-Y",
        "colab_type": "text"
      },
      "source": [
        "# **Reccurent Neural Network from scratch**\n",
        "**Understanding the mathematics behind RNNs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXOBbGJHtpw",
        "colab_type": "text"
      },
      "source": [
        "Importing Numpy for matrix math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U9uiWkGAZU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy, numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMluVhhHH10N",
        "colab_type": "text"
      },
      "source": [
        "**Reading Data From Kafka.txt creating a dictinoary map of characters to integer(or their position) in total number of unique characters in data and\n",
        "and integer(or their position) in total number of data to its accosiated characters and one_hot_encoding of the character**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIes4gYOC1qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('kafka.txt','r').read()\n",
        "chars = list(set(data))\n",
        "n = 2\n",
        "data_size , unique_size = len(data),len(chars)\n",
        "char_ix = {ch:i for i,ch in enumerate(chars)}\n",
        "ix_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3knkgSIlXu",
        "colab_type": "text"
      },
      "source": [
        "Weight matrix Initialization for : \n",
        "\n",
        "\n",
        "1.   Input to Current Hidden Layer \n",
        "2.   Current Hidden to Previous Hidden Layer\n",
        "3.   Merged Current Hidden to output\n",
        "\n",
        "and biases \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSrHTRfJGl6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 100\n",
        "seq_length = 500\n",
        "\n",
        "wxh = np.random.randn(hidden_size, len(chars)) * 0.01\n",
        "whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "why = np.random.randn(len(chars), hidden_size) * 0.01 #input to hidden\n",
        "bh = np.zeros((hidden_size,1))\n",
        "by = np.zeros((len(chars),1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcIpEJtIB9y_",
        "colab_type": "code",
        "outputId": "929955c5-09cf-4f6f-9f0d-aff347ff9cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wxh.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MDXjibdJIOL",
        "colab_type": "text"
      },
      "source": [
        "Example of One_Hot_Encoding Of Character to Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRGWJPXH5a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_for_a = np.zeros((len(chars),1))\n",
        "vector_for_a[char_ix['a']] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVD64O6jJRo-",
        "colab_type": "text"
      },
      "source": [
        "Feed Forward of RNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HmmP8yIoPbf",
        "colab_type": "text"
      },
      "source": [
        "Creating a dataset of the position of characters mapping it to next upcoming character in kafka.txt dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7P7Ur7zHHFy",
        "colab_type": "code",
        "outputId": "cbf3f620-3ce6-43b6-dd5a-efd1f92335b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "    \n",
        "\n",
        "def train(x,y,hprev):\n",
        "    xs , hs , ys , ps = {}, {} , {} , {}\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "    \n",
        "    #Forward Propagation\n",
        "    for i in range(seq_length):\n",
        "        \n",
        "        xs[i] = np.zeros((len(chars),1))\n",
        "        xs[i][x[i]] = 1\n",
        "        hs[i] = np.tanh(np.dot(wxh,xs[i]) + np.dot(whh,hs[i-1]) + bh)\n",
        "        \n",
        "        ys[i] = np.dot(why,hs[i])+ by\n",
        "        ps[i] = np.exp(ys[i])/np.sum(np.exp(ys[i]))\n",
        "       \n",
        "        loss += -np.log(ps[i][y[i]])\n",
        "        \n",
        "    \n",
        "    #Back Propagation Through Time (BPTT)\n",
        "    dwxh , dwhh , dwhy , dbh , dby = np.zeros_like(wxh) , np.zeros_like(whh) , np.zeros_like(why) , np.zeros_like(bh) ,np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "    for i in reversed(range(len(x))):\n",
        "        #Updating Gradients calculating small gradients at every time step\n",
        "        dy = np.copy(ps[i])\n",
        "        dy[y[i]] -= 1\n",
        "        \n",
        "        dwhy += np.dot(dy,hs[i].T)\n",
        "        \n",
        "        dby += dy\n",
        "        \n",
        "        \n",
        "        dh = np.dot(why.T,dy) + dhnext\n",
        "        \n",
        "        dhraw = (1 - hs[i] * hs[i]) * dh\n",
        "        \n",
        "        dbh +=  dhraw\n",
        "        dwxh += np.dot(dhraw,xs[i].T)\n",
        "        dwhh += np.dot(dhraw,hs[i-1].T)\n",
        "       \n",
        "        dhnext = np.dot(whh.T,dhraw)\n",
        "        \n",
        "    for dparam in [dwxh, dwhh, dwhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "    return loss, dwxh, dwhh, dwhy, dbh, dby, hs[seq_length-1]\n",
        "        \n",
        "def predict(seed_x,h):\n",
        "    \n",
        "    x = np.zeros((len(chars),1))\n",
        "    x[seed_x] = 1\n",
        "    vec_outputs = list()\n",
        "    yguess = list()\n",
        "    \n",
        "    for i in range(seq_length):\n",
        "        #Input * whx +  prev_hidden * whh + bh ==> new_merged_hidden\n",
        "        h = np.tanh(np.dot(wxh,x) + np.dot(whh,h) + bh)\n",
        "        \n",
        "        #new_merged_hidden * why + by ==> yhat\n",
        "        y = np.dot(why,h) + by\n",
        "        \n",
        "        #normalized(yhat) ==> normalized_output\n",
        "        p = np.exp(y)/np.sum(np.exp(y))\n",
        "       \n",
        "        #new_vecx == normalized_output == new_x\n",
        "        #Choose with highest probability\n",
        "        ix = np.argmax(p)\n",
        "        x = np.zeros((len(chars),1))\n",
        "        x[ix] = 1\n",
        "        yguess.append(ix)\n",
        "        \n",
        "    \n",
        "    \n",
        "    print(''.join([ix_char[c] for c in yguess]))\n",
        "\n",
        "x = [char_ix[c] for c in data[0:seq_length]]\n",
        "hprev = np.zeros((hidden_size,1))\n",
        "predict(x[0],hprev)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "J!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sgZ5zuTdB6k",
        "colab_type": "code",
        "outputId": "d4fd5870-585a-4c72-dbde-0cbc228fe83f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1288
        }
      },
      "source": [
        "   \n",
        "x = [char_ix[c] for c in data[0:seq_length]]\n",
        "y = [char_ix[c] for c in data[1:seq_length+1]]\n",
        "print('Given X:')\n",
        "print(x[0])\n",
        "print(ix_char[x[0]])\n",
        "print('Known Y:')\n",
        "print(y)\n",
        "print(len(y))\n",
        "print(''.join([ix_char[c] for c in y]))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "nses , pses = 0,0\n",
        "lr = 1e-1\n",
        "mWxh, mWhh, mWhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/len(chars))*seq_length # loss at iteration 0 \n",
        "\n",
        "for i in range(1000*100):\n",
        "    if pses+seq_length+1 >= len(data) or nses == 0:\n",
        "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "        pses = 0 # go from start of data\n",
        "    x = [char_ix[c] for c in data[0:seq_length]]\n",
        "    y = [char_ix[c] for c in data[1:seq_length+1]]\n",
        "    loss, dwxh, dwhh, dwhy, dbh, dby, hprev  = train(x,y,hprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    if i % 1000 == 0:\n",
        "        predict(x[0],hprev)\n",
        "        print('Iter:{1}   Loss:{0}'.format(smooth_loss,i))\n",
        "    for param, dparam, mem in zip([wxh, whh, why, bh, by],\n",
        "                                [dwxh, dwhh, dwhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -lr * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given X:\n",
            "45\n",
            "O\n",
            "Known Y:\n",
            "[34, 22, 48, 4, 51, 31, 34, 47, 34, 37, 74, 48, 66, 7, 22, 34, 48, 57, 31, 22, 37, 51, 31, 48, 72, 46, 4, 15, 46, 48, 66, 51, 14, 22, 48, 20, 31, 51, 4, 48, 32, 31, 51, 11, 64, 2, 22, 78, 48, 78, 31, 22, 46, 4, 15, 74, 48, 7, 22, 48, 20, 51, 11, 34, 78, 48, 7, 47, 4, 15, 22, 2, 20, 48, 32, 31, 46, 34, 15, 20, 51, 31, 4, 22, 78, 48, 47, 34, 48, 7, 47, 15, 48, 64, 22, 78, 48, 47, 34, 32, 51, 48, 46, 48, 7, 51, 31, 31, 47, 64, 2, 22, 48, 52, 22, 31, 4, 47, 34, 63, 48, 43, 22, 48, 2, 46, 30, 48, 51, 34, 48, 7, 47, 15, 48, 46, 31, 4, 51, 11, 31, 79, 2, 47, 14, 22, 48, 64, 46, 71, 14, 74, 48, 46, 34, 78, 48, 47, 20, 48, 7, 22, 48, 2, 47, 20, 32, 22, 78, 48, 7, 47, 15, 48, 7, 22, 46, 78, 48, 46, 48, 2, 47, 32, 32, 2, 22, 48, 7, 22, 48, 71, 51, 11, 2, 78, 48, 15, 22, 22, 48, 7, 47, 15, 48, 64, 31, 51, 66, 34, 48, 64, 22, 2, 2, 30, 74, 48, 15, 2, 47, 37, 7, 32, 2, 30, 48, 78, 51, 4, 22, 78, 48, 46, 34, 78, 48, 78, 47, 52, 47, 78, 22, 78, 48, 64, 30, 48, 46, 31, 71, 7, 22, 15, 48, 47, 34, 32, 51, 48, 15, 32, 47, 20, 20, 48, 15, 22, 71, 32, 47, 51, 34, 15, 63, 48, 65, 7, 22, 48, 64, 22, 78, 78, 47, 34, 37, 48, 66, 46, 15, 48, 7, 46, 31, 78, 2, 30, 48, 46, 64, 2, 22, 48, 32, 51, 48, 71, 51, 52, 22, 31, 48, 47, 32, 48, 46, 34, 78, 48, 15, 22, 22, 4, 22, 78, 48, 31, 22, 46, 78, 30, 48, 32, 51, 48, 15, 2, 47, 78, 22, 48, 51, 20, 20, 48, 46, 34, 30, 48, 4, 51, 4, 22, 34, 32, 63, 48, 43, 47, 15, 48, 4, 46, 34, 30, 48, 2, 22, 37, 15, 74, 48, 36, 47, 32, 47, 20, 11, 2, 2, 30, 48, 32, 7, 47, 34, 48, 71, 51, 4, 36, 46, 31, 22, 78, 48, 66, 47, 32, 7, 48, 32, 7, 22, 48, 15, 47, 58, 22, 48, 51, 20, 48, 32, 7, 22, 48, 31, 22, 15, 32, 48, 51, 20, 48, 7, 47, 4, 74, 48, 66, 46, 52, 22, 78, 48, 46, 64, 51, 11, 32, 48, 7, 22, 2, 36, 2, 22, 15, 15, 2, 30, 48, 46, 15, 48, 7, 22, 48, 2, 51, 51, 14, 22, 78, 63, 24, 24, 54, 18, 7, 46, 32, 59, 15, 48, 7, 46, 36, 36, 22, 34, 22, 78, 48, 32, 51, 48, 4, 22, 5, 54, 48, 7, 22, 48, 32, 7, 51]\n",
            "500\n",
            "ne morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he tho\n",
            "\n",
            "\n",
            "7iw.\n",
            "0Hn jTLQ4OJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!m'cGNJ!\n",
            "Iter:0   Loss:[2191.01332359]\n",
            "t on hif ared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thormed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he look\n",
            "Iter:1000   Loss:[1024.26479138]\n",
            "n co pared wamed as herkitifnd hen Gregd beemses in comses if hesly Hitiff ded sed as se seare,freseiff tion trof le s beemed as seeseele t. Hisees. tif le was. His belegon His  s, Hele toons. s ze tiok d s be seed seefeele to toun tion tion tron co liok,oks ho led anto tled de tibe l. Hiff to toonseiverkitof areang, ms he le tion trof le tion tion trof le tion trof le tion trof le tion tion trof le tion trof le tion trof le tion tion trof le tion trof le tion trof le tion tion trof le tion trof\n",
            "Iter:2000   Loss:[460.7579158]\n",
            "tion cof areleded wn bedd as he co cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, whtlid of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, whtlid of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, whtlid of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, whtl\n",
            "Iter:3000   Loss:[211.23224481]\n",
            "tion his bed into a hof and in tran his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, whtlid of him, waved about helplessly as he looke\n",
            "Iter:4000   Loss:[85.29234153]\n",
            "tioved anto n. Hipp hact to tran ff thons. He ly th trany, sgons, an ans. Thelofue ve sezedomed and domed and sf ly domed an he ly thse ff the seike back, tiok He ly th trany, sgonsl.y ant. The offtiff wn hised an hifteen. He ly th trany lr he to ths lay ay ango whtt ango wasly thif he lele s. His homla wo bed in trany, sgonsl.yeark, whef is mo wislike off in trany, sgonsl. thongof h sd wamed..selofme was he lay ongheareams, wn his bedd of to t. His hittofutly thon heslike ff tik, whtlly thin hi\n",
            "Iter:5000   Loss:[117.14178483]\n",
            "n cesli to slided by arches helly are teleled and  bound sectians. The his bed in troublifreapld His haad o m, wheams, and in his armoubled thea s, pitiff helple to coveree his haad o m, wheams, wn his haa wi histoformed in his armoubleghe pittled and seimin ctited wheam hearmoubly able to coved and divided s thithing, Thelrmo whearlod hely and ind sed domed and sablided wn his bany loke bedtis ared with she hislittomseiff harrhif ind sed ifftsloke bedtised any wasly as heams, whtlith seimed rea\n",
            "Iter:6000   Loss:[133.64885932]\n",
            "tion his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thoms, when Gregor Samsa woke from troubled dreams, he found himself transformed in his\n",
            "Iter:7000   Loss:[90.53240193]\n",
            "tiff secd off se?\" he thoms, when ched oubled thed into a his brown belly, slight.\n",
            "\n",
            "\"What's happened to ch, he could any legs, piti tifteams, and if his hardinghelp n compareaby then he could stifully thin ver into s se?\" he the lifted his head a lifted his head a little he could stifully thin ver into s se?\" he the lifted his head a lifted his head a little he could stifully thin ver into s se?\" he the lifted his head a lifted his head a little he could stifully thin ver into s se?\" he the lift\n",
            "Iter:8000   Loss:[48.24929012]\n",
            "tioned s  he looked.\n",
            "\n",
            "\"What's happened to me?\" he thom aroully he lo tion his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
            "\n",
            "\"What's happened to me?\" he thom aroully he lo\n",
            "Iter:9000   Loss:[22.00650819]\n",
            "mstran ay whe the his he ler ittouple themed if he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if he he sezream ans, Hhe tif he he sezream and if\n",
            "Iter:10000   Loss:[334.12315291]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-247a0f7040b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m  \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m  \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdwxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdwhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdwhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m  \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-54b2de4b26d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, hprev)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwxh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3q0Gw_RQg7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 600\n",
        "\n",
        "x = [char_ix[c] for c in data[506:seq_length]]\n",
        "y = [char_ix[c] for c in data[507:seq_length+1]]\n",
        "print(ix_char[x[1]])\n",
        "print(y)\n",
        "hprev = np.zeros((hidden_size,1))\n",
        "predict(x[1],hprev)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}